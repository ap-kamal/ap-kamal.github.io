# Part 1 - Predicting Review Scores on Pitchfork

For Part 1, we will be using data from [this paper](https://ojs.aaai.org/index.php/ICWSM/article/view/7355). The data is a collection of reviews from [Pitchfork](https://pitchfork.com/), a site that provides expert reviews of music album. The authors of this paper have also combined the data with a set of features from [Spotify’s API](https://developer.spotify.com/documentation/web-api/) that provide insight into the music itself, e.g. the "acousticness" of the song.  We will tackle a regression problem here, trying to predict the score of a review from several of the other columns in the dataset.

## Part 1.1 - Feature Engineering with Feature Subsets

In the first subsection of Part 1, We’re going to look at how running linear regression with various subsets of our features impacts our ability to predict score.

In Part 1.1, Here we are going to train a separate linear regression model for a number of different feature subsets.  Specifically:

- The list `feature_sets` below is a list of lists; each sublist is a different subset of features to build a model with. 
- All models should be trained on the dataset `part1_train.csv`. 
- For each of these trained models, you should evaluate the model’s predictions on the training dataset, as well as the provided test set, called `part1_test.csv`. The evaluation metric we will use is **root mean squared error**.  

Our output file `part_1.1_results.csv` will have the following columns:
- `feature_set` - a column describing the features of the model used. For feature sets with multiple features, combine them using an underscore (you can do this with the code `"_".join(feature_set)`)
- `training_rmse` - a column that gives the RMSE of a linear regression model trained on this feature set on the training data
- `test_rmse` - a column that gives the RMSE of a linear regression model trained on this feature set on the test data


```python
feature_sets = [['artist'],
 ['reviewauthor'],
 ['releaseyear'],
 ['recordlabel'],
 ['genre'],
 ['danceability'],
 ['energy'],
 ['key'],
 ['loudness'],
 ['speechiness'],
 ['acousticness'],
 ['instrumentalness'],
 ['liveness'],
 ['valence'],
 ['tempo'],
 ['danceability','energy','key','loudness','speechiness','acousticness',
  'instrumentalness','liveness','valence','tempo'],
 ['artist', 'reviewauthor', 'releaseyear', 'recordlabel', 'genre'],
 ['artist', 'reviewauthor', 'releaseyear', 'recordlabel', 'genre', 'danceability', 
  'energy', 'key', 'loudness', 'speechiness', 'acousticness', 'instrumentalness',
  'liveness', 'valence', 'tempo']]

```


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import math
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

labelencoder = LabelEncoder()

# Convenience things for you, note that releaseyear is continuous but is not a Spotify API variable
CONTINUOUS_FEATURES = ['releaseyear', 'danceability', 'energy', 'key', 'loudness',
       'speechiness', 'acousticness', 'instrumentalness', 'liveness',
       'valence', 'tempo']
CATEGORICAL_FEATURES = ['artist', 'reviewauthor', 'recordlabel', 'genre']

# Read in the data
training_data = pd.read_csv("part1_train.csv")
test_data = pd.read_csv("part1_test.csv")

new_training_data = training_data
new_test_data = test_data

for feature in CATEGORICAL_FEATURES:
    new_training_data[feature] = labelencoder.fit_transform(new_training_data[feature])
    new_test_data[feature] = labelencoder.fit_transform(new_test_data[feature])

result_data = []
for feature_set in feature_sets:
    X_train = new_training_data[feature_set]
    y_train = new_training_data['score']    
    X_test = new_test_data[feature_set]
    y_test = new_test_data['score']
    clf1 = LinearRegression(fit_intercept=True)
    clf1.fit(X_train,y_train)
    test_pred = clf1.predict(X_test)
    train_pred = clf1.predict(X_train)
    result_data.append(["_".join(feature_set), math.sqrt(mean_squared_error(y_train,train_pred)), math.sqrt(mean_squared_error(y_test,test_pred))])

result = pd.DataFrame(result_data)
result.columns = ["feature_set","training_rmse","test_rmse"]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_set</th>
      <th>training_rmse</th>
      <th>test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>artist</td>
      <td>1.243393</td>
      <td>1.243591</td>
    </tr>
    <tr>
      <th>1</th>
      <td>reviewauthor</td>
      <td>1.243345</td>
      <td>1.243498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>releaseyear</td>
      <td>1.235994</td>
      <td>1.232989</td>
    </tr>
    <tr>
      <th>3</th>
      <td>recordlabel</td>
      <td>1.242969</td>
      <td>1.244257</td>
    </tr>
    <tr>
      <th>4</th>
      <td>genre</td>
      <td>1.242326</td>
      <td>1.242353</td>
    </tr>
    <tr>
      <th>5</th>
      <td>danceability</td>
      <td>1.240362</td>
      <td>1.241097</td>
    </tr>
    <tr>
      <th>6</th>
      <td>energy</td>
      <td>1.240900</td>
      <td>1.239929</td>
    </tr>
    <tr>
      <th>7</th>
      <td>key</td>
      <td>1.243497</td>
      <td>1.243823</td>
    </tr>
    <tr>
      <th>8</th>
      <td>loudness</td>
      <td>1.236968</td>
      <td>1.237578</td>
    </tr>
    <tr>
      <th>9</th>
      <td>speechiness</td>
      <td>1.243530</td>
      <td>1.243566</td>
    </tr>
    <tr>
      <th>10</th>
      <td>acousticness</td>
      <td>1.240370</td>
      <td>1.240188</td>
    </tr>
    <tr>
      <th>11</th>
      <td>instrumentalness</td>
      <td>1.238989</td>
      <td>1.238840</td>
    </tr>
    <tr>
      <th>12</th>
      <td>liveness</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>13</th>
      <td>valence</td>
      <td>1.242268</td>
      <td>1.240970</td>
    </tr>
    <tr>
      <th>14</th>
      <td>tempo</td>
      <td>1.243195</td>
      <td>1.242322</td>
    </tr>
    <tr>
      <th>15</th>
      <td>danceability_energy_key_loudness_speechiness_a...</td>
      <td>1.233091</td>
      <td>1.235554</td>
    </tr>
    <tr>
      <th>16</th>
      <td>artist_reviewauthor_releaseyear_recordlabel_genre</td>
      <td>1.233341</td>
      <td>1.231714</td>
    </tr>
    <tr>
      <th>17</th>
      <td>artist_reviewauthor_releaseyear_recordlabel_ge...</td>
      <td>1.224337</td>
      <td>1.224779</td>
    </tr>
  </tbody>
</table>
</div>




```python
result.to_csv("part_1.1_results.csv",sep = '\t')
```

## Part 1.2 - Feature Engineering with the LASSO

In Part 1.2, We will be training an L1-regularized linear regression model, with an expanded feature set.  Specifically:

1. Begin with the final feature set listed in `feature_sets` (i.e. your feature set, to begin this section, is `feature_sets[-1]`.
2. One-hot encode your categorical variables, setting `drop=if_binary` and `sparse=False` in the function arguments. 
3. Scale all of your continuous features using the `StandardScaler`.
4. Train an L1-regularized linear regression model using these features on the dataset `part1_train.csv`. You should use the [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) class in `sklearn`, it will do the cross-validation necessary to select the appropriate value for the regularizer for you!  Use 10-fold cross-validation to perform model selection (set the `LassoCV` parmaeter `cv` to 10), and set the `random_state` to 1. Do not change any of the other parameters to `LassoCV` (i.e. leave them at their defaults).
5. Identify the best `alpha` value (the regularizer term, according to `sklearn`. In class, we refer to this as $\lambda$!) in terms of average mean squared error according to the cross-validation.
6. Finally, train a [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model on the entire training dataset (`part1_train.csv`). We will use this to report the root mean squared error on the test set.


```python
# Write your code for Part 1.2 here
from sklearn.linear_model import LassoCV, Lasso
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix,classification_report, precision_score, roc_auc_score

# Do the CV to find alpha
features = feature_sets[-1]
X_train = training_data[features]
y_train = training_data['score']    
X_test = test_data[features]
y_test = test_data['score']
basic_pipeline = make_pipeline(
        ColumnTransformer([('numerical', StandardScaler(), CONTINUOUS_FEATURES),
                           ("categorical", OneHotEncoder(drop ="if_binary",sparse=False),CATEGORICAL_FEATURES)]),
    )

clf1 = LassoCV(cv = 10, random_state = 1)
pipe1 = Pipeline([("pipe",basic_pipeline),
                  ('clf1', clf1)])
pipe1.fit(X_train,y_train)
y_pred_test =  pipe1.predict(X_test)
y_pred_train = pipe1.predict(X_train)

rmse_train = math.sqrt(mean_squared_error(y_train,y_pred_train))
rmse_test = math.sqrt(mean_squared_error(y_test,y_pred_test))

                    
evaluation_matrix_base_training = {
    "training" : [rmse_train],
    "testing": [rmse_test]
}
    

eva_base_train = pd.DataFrame(data = evaluation_matrix_base_training, index = ['RMSE'])
display(eva_base_train)


# Retrain the model
l1_penalties = np.logspace(-3, 3, num = 15) # TODO: make the list of lambda values
lasso_output = []
lasso_data_train = training_data[features + ["score"]].dropna()
lasso_data_test = test_data[features+["score"]].dropna()
for l1_penalty in l1_penalties:
    regression_pipeline = make_pipeline(ColumnTransformer([
                           ("categorical", OneHotEncoder(drop ="if_binary",sparse=False),CATEGORICAL_FEATURES),
                           ("scale",StandardScaler(),CONTINUOUS_FEATURES)
                          ]),
                          Lasso(alpha=l1_penalty, random_state=1)
                         ) 
    regression_pipeline.fit(lasso_data_train,lasso_data_train.score)

    train_rmse = np.sqrt(mean_squared_error(lasso_data_train.score, 
                                            regression_pipeline.predict(lasso_data_train)))
    test_rmse = np.sqrt(mean_squared_error(lasso_data_test.score, 
                                            regression_pipeline.predict(lasso_data_test)))
    # We maintain a list of dictionaries containing our results
    lasso_output.append({'l1_penalty': l1_penalty,'model': regression_pipeline,'train_rmse': train_rmse,'test_rmse': test_rmse})
    
lasso_output = pd.DataFrame(lasso_output)
display(lasso_output)

```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>training</th>
      <th>testing</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>RMSE</th>
      <td>1.122921</td>
      <td>1.343646</td>
    </tr>
  </tbody>
</table>
</div>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>l1_penalty</th>
      <th>model</th>
      <th>train_rmse</th>
      <th>test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.001000</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.164865</td>
      <td>1.249920</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.002683</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.193347</td>
      <td>1.227617</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.007197</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.212013</td>
      <td>1.224412</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.019307</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.226351</td>
      <td>1.224815</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.051795</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.230498</td>
      <td>1.229373</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.138950</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.372759</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1.000000</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2.682696</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7.196857</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>10</th>
      <td>19.306977</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>11</th>
      <td>51.794747</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>12</th>
      <td>138.949549</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>13</th>
      <td>372.759372</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1000.000000</td>
      <td>(ColumnTransformer(transformers=[('categorical...</td>
      <td>1.243534</td>
      <td>1.243558</td>
    </tr>
  </tbody>
</table>
</div>



```python
(lasso_output["train_rmse"].sum())/15
```




    1.2308278386434948




```python
from sklearn import set_config 
from sklearn.utils import estimator_html_repr 
from IPython.core.display import display, HTML 
set_config(display='diagram')
mod = lasso_output.model.iloc[2]
mod
```




<style>#sk-91657dc7-6302-46db-8000-9f99000c31e9 {color: black;background-color: white;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 pre{padding: 0;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-toggleable {background-color: white;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-item {z-index: 1;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-parallel-item:only-child::after {width: 0;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-91657dc7-6302-46db-8000-9f99000c31e9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-91657dc7-6302-46db-8000-9f99000c31e9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,
                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,
                                                  OneHotEncoder(drop=&#x27;if_binary&#x27;,
                                                                sparse=False),
                                                  [&#x27;artist&#x27;, &#x27;reviewauthor&#x27;,
                                                   &#x27;recordlabel&#x27;, &#x27;genre&#x27;]),
                                                 (&#x27;scale&#x27;, StandardScaler(),
                                                  [&#x27;releaseyear&#x27;,
                                                   &#x27;danceability&#x27;, &#x27;energy&#x27;,
                                                   &#x27;key&#x27;, &#x27;loudness&#x27;,
                                                   &#x27;speechiness&#x27;,
                                                   &#x27;acousticness&#x27;,
                                                   &#x27;instrumentalness&#x27;,
                                                   &#x27;liveness&#x27;, &#x27;valence&#x27;,
                                                   &#x27;tempo&#x27;])])),
                (&#x27;lasso&#x27;, Lasso(alpha=0.0071968567300115215, random_state=1))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="5f214aab-389a-4bee-a569-23bce81133a4" type="checkbox" ><label for="5f214aab-389a-4bee-a569-23bce81133a4" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,
                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,
                                                  OneHotEncoder(drop=&#x27;if_binary&#x27;,
                                                                sparse=False),
                                                  [&#x27;artist&#x27;, &#x27;reviewauthor&#x27;,
                                                   &#x27;recordlabel&#x27;, &#x27;genre&#x27;]),
                                                 (&#x27;scale&#x27;, StandardScaler(),
                                                  [&#x27;releaseyear&#x27;,
                                                   &#x27;danceability&#x27;, &#x27;energy&#x27;,
                                                   &#x27;key&#x27;, &#x27;loudness&#x27;,
                                                   &#x27;speechiness&#x27;,
                                                   &#x27;acousticness&#x27;,
                                                   &#x27;instrumentalness&#x27;,
                                                   &#x27;liveness&#x27;, &#x27;valence&#x27;,
                                                   &#x27;tempo&#x27;])])),
                (&#x27;lasso&#x27;, Lasso(alpha=0.0071968567300115215, random_state=1))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="2d566f08-4b66-49e4-bf07-6109e4c2fc1e" type="checkbox" ><label for="2d566f08-4b66-49e4-bf07-6109e4c2fc1e" class="sk-toggleable__label sk-toggleable__label-arrow">columntransformer: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,
                                 OneHotEncoder(drop=&#x27;if_binary&#x27;, sparse=False),
                                 [&#x27;artist&#x27;, &#x27;reviewauthor&#x27;, &#x27;recordlabel&#x27;,
                                  &#x27;genre&#x27;]),
                                (&#x27;scale&#x27;, StandardScaler(),
                                 [&#x27;releaseyear&#x27;, &#x27;danceability&#x27;, &#x27;energy&#x27;,
                                  &#x27;key&#x27;, &#x27;loudness&#x27;, &#x27;speechiness&#x27;,
                                  &#x27;acousticness&#x27;, &#x27;instrumentalness&#x27;,
                                  &#x27;liveness&#x27;, &#x27;valence&#x27;, &#x27;tempo&#x27;])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="d3510bb4-0447-413c-a5b0-dfa8c662e543" type="checkbox" ><label for="d3510bb4-0447-413c-a5b0-dfa8c662e543" class="sk-toggleable__label sk-toggleable__label-arrow">categorical</label><div class="sk-toggleable__content"><pre>[&#x27;artist&#x27;, &#x27;reviewauthor&#x27;, &#x27;recordlabel&#x27;, &#x27;genre&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="95166263-618f-49f6-8e0a-0f0011ccd55e" type="checkbox" ><label for="95166263-618f-49f6-8e0a-0f0011ccd55e" class="sk-toggleable__label sk-toggleable__label-arrow">OneHotEncoder</label><div class="sk-toggleable__content"><pre>OneHotEncoder(drop=&#x27;if_binary&#x27;, sparse=False)</pre></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="1a034fc3-44b8-4258-b578-63f149012c07" type="checkbox" ><label for="1a034fc3-44b8-4258-b578-63f149012c07" class="sk-toggleable__label sk-toggleable__label-arrow">scale</label><div class="sk-toggleable__content"><pre>[&#x27;releaseyear&#x27;, &#x27;danceability&#x27;, &#x27;energy&#x27;, &#x27;key&#x27;, &#x27;loudness&#x27;, &#x27;speechiness&#x27;, &#x27;acousticness&#x27;, &#x27;instrumentalness&#x27;, &#x27;liveness&#x27;, &#x27;valence&#x27;, &#x27;tempo&#x27;]</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="9fd85a69-40f5-4946-8043-81b0e623d3f3" type="checkbox" ><label for="9fd85a69-40f5-4946-8043-81b0e623d3f3" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="a954052a-cfe0-4bc4-97fb-166095f8f70e" type="checkbox" ><label for="a954052a-cfe0-4bc4-97fb-166095f8f70e" class="sk-toggleable__label sk-toggleable__label-arrow">Lasso</label><div class="sk-toggleable__content"><pre>Lasso(alpha=0.0071968567300115215, random_state=1)</pre></div></div></div></div></div></div></div>




```python
lasso_results = pd.DataFrame({"features" : mod[-2].get_feature_names_out(),
              "coefs" : mod[-1].coef_
             })
lasso_results.sort_values("coefs", ascending = False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>666</th>
      <td>categorical__recordlabel_316</td>
      <td>0.196678</td>
    </tr>
    <tr>
      <th>670</th>
      <td>categorical__genre_1</td>
      <td>0.121404</td>
    </tr>
    <tr>
      <th>287</th>
      <td>categorical__reviewauthor_181</td>
      <td>0.086074</td>
    </tr>
    <tr>
      <th>242</th>
      <td>categorical__reviewauthor_136</td>
      <td>0.085880</td>
    </tr>
    <tr>
      <th>687</th>
      <td>scale__instrumentalness</td>
      <td>0.062686</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>680</th>
      <td>scale__releaseyear</td>
      <td>-0.129545</td>
    </tr>
    <tr>
      <th>63</th>
      <td>categorical__artist_63</td>
      <td>-0.131319</td>
    </tr>
    <tr>
      <th>301</th>
      <td>categorical__reviewauthor_195</td>
      <td>-0.171756</td>
    </tr>
    <tr>
      <th>108</th>
      <td>categorical__reviewauthor_2</td>
      <td>-0.189233</td>
    </tr>
    <tr>
      <th>187</th>
      <td>categorical__reviewauthor_81</td>
      <td>-0.303939</td>
    </tr>
  </tbody>
</table>
<p>691 rows × 2 columns</p>
</div>




```python
non_zeros_count = sum(lasso_results["coefs"][lasso_results["coefs"] != 0].value_counts())
total_count = sum(lasso_results["coefs"].value_counts())
print(f'Total number of non zero coefficients are',non_zeros_count)
print(f'Percentage of total number of non zero coefficients are',round((non_zeros_count/total_count),4)*100)
```

    Total number of non zero coefficients are 21
    Percentage of total number of non zero coefficients are 3.04



```python
lasso_results[lasso_results["features"].str.contains('reviewauthor', na=False)].sort_values("coefs", ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>287</th>
      <td>categorical__reviewauthor_181</td>
      <td>0.086074</td>
    </tr>
    <tr>
      <th>242</th>
      <td>categorical__reviewauthor_136</td>
      <td>0.085880</td>
    </tr>
    <tr>
      <th>273</th>
      <td>categorical__reviewauthor_167</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>261</th>
      <td>categorical__reviewauthor_155</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>262</th>
      <td>categorical__reviewauthor_156</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>349</th>
      <td>categorical__reviewauthor_243</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>236</th>
      <td>categorical__reviewauthor_130</td>
      <td>-0.121298</td>
    </tr>
    <tr>
      <th>301</th>
      <td>categorical__reviewauthor_195</td>
      <td>-0.171756</td>
    </tr>
    <tr>
      <th>108</th>
      <td>categorical__reviewauthor_2</td>
      <td>-0.189233</td>
    </tr>
    <tr>
      <th>187</th>
      <td>categorical__reviewauthor_81</td>
      <td>-0.303939</td>
    </tr>
  </tbody>
</table>
<p>244 rows × 2 columns</p>
</div>




```python
lasso_results[lasso_results["features"].str.contains('genre', na=False)].sort_values("coefs", ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>670</th>
      <td>categorical__genre_1</td>
      <td>0.121404</td>
    </tr>
    <tr>
      <th>671</th>
      <td>categorical__genre_2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>672</th>
      <td>categorical__genre_3</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>673</th>
      <td>categorical__genre_4</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>674</th>
      <td>categorical__genre_5</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>675</th>
      <td>categorical__genre_6</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>676</th>
      <td>categorical__genre_7</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>678</th>
      <td>categorical__genre_9</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>677</th>
      <td>categorical__genre_8</td>
      <td>-0.037467</td>
    </tr>
    <tr>
      <th>669</th>
      <td>categorical__genre_0</td>
      <td>-0.096707</td>
    </tr>
    <tr>
      <th>679</th>
      <td>categorical__genre_10</td>
      <td>-0.118597</td>
    </tr>
  </tbody>
</table>
</div>




```python
lasso_results[lasso_results["features"].str.contains('artist', na=False)].sort_values("coefs", ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>categorical__artist_0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>categorical__artist_1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>78</th>
      <td>categorical__artist_78</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>77</th>
      <td>categorical__artist_77</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>76</th>
      <td>categorical__artist_76</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>31</th>
      <td>categorical__artist_31</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>30</th>
      <td>categorical__artist_30</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>29</th>
      <td>categorical__artist_29</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>105</th>
      <td>categorical__artist_105</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>63</th>
      <td>categorical__artist_63</td>
      <td>-0.131319</td>
    </tr>
  </tbody>
</table>
<p>106 rows × 2 columns</p>
</div>



# Part 1.4 - "Manual" Cross-Validation + Holdout for Model Selection and Evaluation

We will finally use cross validation for both algorithm and model selection, with a hold-out test set for a final evaluation. We will use **5-fold cross validation** to identify the best parameters and hyperparameters for a set of models. We will then take our final models and use a final hold-out test set (the same one as above) to estimate the generalization error of the models.

Specifically, We will be training and evaluating the following models, one for each of the specified hyper parameters sets:

- `Decision Tree regression` - All combinations of a `max_depth` of 5, 10, or 20, and a `criterion` of `"squared error"` or `"absolute error"`. Use the [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor).
- Ridge regression - Use the following choices of L2 penalty: $[10^{-5}, 10^{-4}, ..., 10^4, 10^5]$. In Python, you can create a list of these numbers using `np.logspace(-5, 5, 11)`. Use the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class from sklearn to train a Ridge Regression model. The parameters you need to pass when constructing the Ridge model are `alpha`, which lets you specify what you want the L2 penalty to be, and `random_state=0` to avoid randomness.
- kNN regression - Values of `n_neighbors` of 1, 5, 10, and 15. Use the [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) class.

Our output file `part_1.4_results.csv`should have the following columns:
- `model_name` - The name of the model, one of `DTR` (Decision Tree Regression), `Ridge`, or `KNN`.
- `hyperparameter_setting` - a column describing the hyperparameters of the model. For models with multiple hyperparameters, combine them using an underscore (you can do this with the code `"_".join(hyperparameters)`).
- `mean_training_rmse` - a column that gives the mean RMSE on the k-fold training data. You should take the average of the model’s errors on the different folds, using root mean squared error again as your evaluation metric.
- `sd_training_rmse` - a column that gives the standard deviation RMSE on the k-fold training data.
- `test_rmse` - a column that gives the RMSE of a linear regression model trained on this feature set on the test data


```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix,classification_report, precision_score, roc_auc_score

fold0_data = pd.read_csv("/Users/aparna_kamal/Downloads/assignment2/1.2_fold0.csv").dropna()
fold1_data = pd.read_csv("/Users/aparna_kamal/Downloads/assignment2/1.2_fold1.csv").dropna()
fold2_data = pd.read_csv("/Users/aparna_kamal/Downloads/assignment2/1.2_fold2.csv").dropna()
fold3_data = pd.read_csv("/Users/aparna_kamal/Downloads/assignment2/1.2_fold3.csv").dropna()
fold4_data = pd.read_csv("/Users/aparna_kamal/Downloads/assignment2/1.2_fold4.csv").dropna()


fold1234_train = pd.concat([fold1_data,fold2_data,fold3_data,fold4_data])
fold0234_train = pd.concat([fold0_data,fold2_data,fold3_data,fold4_data])
fold0134_train = pd.concat([fold0_data,fold1_data,fold3_data,fold4_data])
fold0123_train = pd.concat([fold1_data,fold2_data,fold3_data,fold0_data])
fold0124_train = pd.concat([fold1_data,fold2_data,fold0_data,fold0_data])

training_entire_set =[]
test_entire_set = []
training_entire_set.append(fold1234_train)
test_entire_set.append(fold0_data)
training_entire_set.append(fold0234_train)
test_entire_set.append(fold1_data)
training_entire_set.append(fold0134_train)
test_entire_set.append(fold2_data)
training_entire_set.append(fold0124_train)
test_entire_set.append(fold3_data)
training_entire_set.append(fold0123_train)
test_entire_set.append(fold4_data)

basic_pipeline = make_pipeline(
        ColumnTransformer([('numerical', StandardScaler(), CONTINUOUS_FEATURES),
                           ("categorical", OneHotEncoder(drop ="if_binary",sparse=False),CATEGORICAL_FEATURES)]),
    )



# clf1 = DecisionTreeRegressor(random_state=1)
# clf2 = Ridge(random_state = 0)
# clf3 = KNeighborsRegressor()

DT_CRITERIA = ["squared_error","absolute_error"]
DT_MAX_DEPTH = [5,10,20]

KNN_NEIGHBOR = [1,2,5,10]
RIDGE_PARAM = np.logspace(-5, 5, num = 11)

result = []

for i in range(0,len(DT_MAX_DEPTH)):
    for j in range(0,len(DT_CRITERIA)):
        rmse_train=0
        rmse_test = 0
        rmse_sd = []
        for k in range(0,5):
            X_train = training_entire_set[k][features]
            y_train = training_entire_set[k]['score']    
            X_test = test_entire_set[k][features]
            y_test = test_entire_set[k]['score']
            clf1 =  DecisionTreeRegressor(random_state = 1, criterion = DT_CRITERIA[j], max_depth=DT_MAX_DEPTH[i])
            pipe_DT = Pipeline([("pipe",basic_pipeline),
                        ('clf1', clf1)])
            pipe_DT.fit(X_train,y_train)
            y_pred_train = pipe_DT.predict(X_train)
            y_pred_test = pipe_DT.predict(X_test)
            rmse_train = math.sqrt(mean_squared_error(y_train,y_pred_train))
            rmse_sd.append(rmse_train)
            rmse_test = rmse_test+math.sqrt(mean_squared_error(y_test,y_pred_test))
        rmse_train_mean = np.mean(rmse_sd)
        rmse_sd_train = np.std(rmse_sd)
        rmse_test = rmse_test/5
        result.append(["DTR",str(DT_MAX_DEPTH[i])+"_"+DT_CRITERIA[j],rmse_train_mean,rmse_sd_train,rmse_test])
            
        
        
### KNN #####
for j in range(0,len(KNN_NEIGHBOR)):
    rmse_train=0
    rmse_test = 0
    for i in range(0,5):
        X_train = training_entire_set[i][features]
        y_train = training_entire_set[i]['score']    
        X_test = test_entire_set[i][features]
        y_test = test_entire_set[i]['score']
        clf3 = KNeighborsRegressor(n_neighbors=KNN_NEIGHBOR[j])
        pipe_KNN = Pipeline([("pipe",basic_pipeline),
                    ('clf3', clf3)])
        pipe_KNN.fit(X_train,y_train)
        y_pred_train = pipe_KNN.predict(X_train)
        y_pred_test = pipe_KNN.predict(X_test)
        rmse_train = math.sqrt(mean_squared_error(y_train,y_pred_train))
        rmse_sd.append(rmse_train)
        rmse_test = rmse_test+math.sqrt(mean_squared_error(y_test,y_pred_test))
    rmse_train_mean = np.mean(rmse_sd)
    rmse_sd_train = np.std(rmse_sd)
    rmse_test = rmse_test/5
    result.append(["KNN",str(KNN_NEIGHBOR[j]),rmse_train_mean,rmse_sd_train,rmse_test])
        

##### RIDGE #####
RIDGE={}
for j in range(0,len(RIDGE_PARAM)):
    rmse_train=0
    rmse_test = 0
    for i in range(0,5):
        X_train = training_entire_set[i][features]
        y_train = training_entire_set[i]['score']    
        X_test = test_entire_set[i][features]
        y_test = test_entire_set[i]['score']
        clf2 = Ridge(alpha = RIDGE_PARAM[j])
        pipe_RR = Pipeline([("pipe",basic_pipeline),
                  ('clf2', clf2)])
        pipe_RR.fit(X_train,y_train)
        y_pred_train = pipe_RR.predict(X_train)
        y_pred_test = pipe_RR.predict(X_test)
        rmse_train = math.sqrt(mean_squared_error(y_train,y_pred_train))
        rmse_sd.append(rmse_train)
        rmse_test = rmse_test+math.sqrt(mean_squared_error(y_test,y_pred_test))
    rmse_train_mean = np.mean(rmse_sd)
    rmse_sd_train = np.std(rmse_sd)
    rmse_test = rmse_test/5
    result.append(['Ridge',str(RIDGE_PARAM[j]),rmse_train_mean,rmse_sd_train,rmse_test])

output = pd.DataFrame(result)
output.columns = ["Model_name","hyperparameter_setting","mean_training_rmse","sd_training_rmse","test_rmse"]
output
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model_name</th>
      <th>hyperparameter_setting</th>
      <th>mean_training_rmse</th>
      <th>sd_training_rmse</th>
      <th>test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DTR</td>
      <td>5_squared_error</td>
      <td>1.185240</td>
      <td>0.006366</td>
      <td>1.209052</td>
    </tr>
    <tr>
      <th>1</th>
      <td>DTR</td>
      <td>5_absolute_error</td>
      <td>1.207012</td>
      <td>0.007595</td>
      <td>1.229086</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DTR</td>
      <td>10_squared_error</td>
      <td>1.107777</td>
      <td>0.013955</td>
      <td>1.250543</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DTR</td>
      <td>10_absolute_error</td>
      <td>1.148851</td>
      <td>0.011303</td>
      <td>1.264720</td>
    </tr>
    <tr>
      <th>4</th>
      <td>DTR</td>
      <td>20_squared_error</td>
      <td>0.947981</td>
      <td>0.019125</td>
      <td>1.348417</td>
    </tr>
    <tr>
      <th>5</th>
      <td>DTR</td>
      <td>20_absolute_error</td>
      <td>0.994747</td>
      <td>0.032185</td>
      <td>1.340303</td>
    </tr>
    <tr>
      <th>6</th>
      <td>KNN</td>
      <td>1</td>
      <td>0.504337</td>
      <td>0.490956</td>
      <td>1.600402</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>2</td>
      <td>0.586845</td>
      <td>0.420960</td>
      <td>1.416636</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN</td>
      <td>5</td>
      <td>0.693795</td>
      <td>0.409158</td>
      <td>1.274647</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN</td>
      <td>10</td>
      <td>0.774806</td>
      <td>0.400271</td>
      <td>1.226887</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Ridge</td>
      <td>1e-05</td>
      <td>0.830618</td>
      <td>0.386141</td>
      <td>1.180388</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Ridge</td>
      <td>0.0001</td>
      <td>0.870485</td>
      <td>0.370612</td>
      <td>1.180387</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Ridge</td>
      <td>0.001</td>
      <td>0.900384</td>
      <td>0.355603</td>
      <td>1.180381</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Ridge</td>
      <td>0.01</td>
      <td>0.923639</td>
      <td>0.341673</td>
      <td>1.180317</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Ridge</td>
      <td>0.1</td>
      <td>0.942245</td>
      <td>0.328924</td>
      <td>1.179707</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Ridge</td>
      <td>1.0</td>
      <td>0.957514</td>
      <td>0.317325</td>
      <td>1.175053</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Ridge</td>
      <td>10.0</td>
      <td>0.971251</td>
      <td>0.307224</td>
      <td>1.165559</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Ridge</td>
      <td>100.0</td>
      <td>0.986336</td>
      <td>0.299770</td>
      <td>1.182385</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Ridge</td>
      <td>1000.0</td>
      <td>1.002310</td>
      <td>0.294558</td>
      <td>1.212416</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Ridge</td>
      <td>10000.0</td>
      <td>1.017486</td>
      <td>0.290187</td>
      <td>1.228917</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Ridge</td>
      <td>100000.0</td>
      <td>1.031621</td>
      <td>0.286265</td>
      <td>1.241454</td>
    </tr>
  </tbody>
</table>
</div>




```python
output.to_csv("part_1.4_results.csv",sep = '\t')
```

# Part 2

Here, we're going to perform optimization of one of the classification models - logistic regression. As a reminder...

The loss function of logistic regression (also known as the logistic-loss or log-loss) is given by:
\begin{equation}
  J({\bf w}) = \frac{1}{n}\sum_{i=1}^n \log{(1 + \exp{(-y_i{\bf w}^\top{\bf x}_i}))}
  \label{eqn:logloss}
\end{equation}

The gradient for this loss function, as derived in class, is:
\begin{equation}
  \nabla J({\bf w}) = -\frac{1}{n}\sum_{i=1}^n \frac{y_i}{1 + \exp{(y_i{\bf w}^\top{\bf x}_i)}}{\bf x}_i
  \label{eqn:loglossgradient}
\end{equation}


The Hessian for the loss function is given by:
\begin{equation}
  {\bf H}({\bf w}) = \frac{1}{n} \sum_{i=1}^n \frac{\exp{(y_i{\bf w}^\top{\bf x}_i)}}{(1 + \exp{(y_i{\bf w}^\top{\bf x}_i)})^2}{\bf x}_i{\bf x}_i^\top
  \label{eqn:loglosshessian}
\end{equation}

## Part 2.1 - Logistic Regression with Gradient Descent

In Part 2.1 we will implement logistic regression with gradient descent. 

1. `logistic_objective` - compute the logistic loss for the given data set (see equation above)
2. `logistic_gradient` - compute the gradient vector of logistic loss for the given data set (see equation above)
3. `run_gradient_descent` - run the gradient descent algorithm, given these two functions.



```python
def logistic_gradient(w, X, y):

    # compute the gradient of the log-loss error (vector) with respect
    # to w (vector) for the given data X and y  
    #
    # Inputs:
    # w = d x 1
    # X = N x d
    # y = N x 1
    # Output:
    # error = d length gradient vector (not a d x 1 matrix)

    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE
    if len(w.shape) == 1:
        w = w[:,np.newaxis]
    gradient = 0
    l = 0
    for i in range(0,len(X)):
        yi=y[i]
        xi=X[i]
        wt=np.transpose(w)
        l = l + (y[i]/(1+np.exp(np.dot(np.dot(-yi,wt),xi))))*(X[i])
    return l/(len(X))

def logistic_objective(w, X, y):

    # compute log-loss error (scalar) with respect
    # to w (vector) for the given data X and y                               
    # Inputs:
    # w = d x 1
    # X = N x d
    # y = N x 1
    # Output:
    # error = scalar
    
    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE
    if len(w.shape) == 1:
        w = w[:,np.newaxis]
    s=0
    for i in range(1,len(X)):
        yi=y[i]
        xi=X[i]
        wt=np.transpose(w)
        s = s + np.log(1+np.exp(np.dot(np.dot(-yi,wt),xi)))
    error = s/len(X)
    return error

def run_gradient_descent(X,y):
    old_w = np.array([-1]*X.shape[1])
    # change this value! This is an unreasonable step size
    step_size = 0.0000005
    new_w =old_w - 1
    
    while ((new_w-old_w)**2).sum() > .0000000001:
        #IMPLEMENT THIS!
        old_w=new_w
        dw = logistic_gradient(new_w, X, y)
        new_w=old_w-(step_size*dw)
    return new_w
```


```python
from scipy.stats import uniform, bernoulli
import functools
draw_binary = functools.partial(np.random.binomial,n=1)

## Simulated data to test your method
DATA_SIZE = 10000
x1 = bernoulli(.5).rvs(DATA_SIZE)
x2 = np.floor(uniform(18,60).rvs(DATA_SIZE))
true_w = [-9, 3.5, 0.2]
xb = true_w[0] + true_w[1]*x1 + true_w[2]*x2
p = 1/(1 + np.exp(-xb))
y = np.array([1 if draw_binary(p=v) else -1 for v in p])
```


```python
from sklearn.linear_model import LogisticRegression

# notice that logistic regression as implemented in sklearn does not get the exact results either!
# so you shouldn't worry if you're a bit off
X = np.hstack([np.ones((len(xb),1)), x1[:,np.newaxis], x2[:,np.newaxis]])
model = LogisticRegression(solver='liblinear', random_state=0,fit_intercept=False)
model.fit(X,y).coef_
```




    array([[-8.48327258,  3.31959859,  0.18859071]])




```python
# this is how we will test your results
gd_result = run_gradient_descent(X,y)
# is your result relatively close to the truth?
np.abs(true_w-gd_result).sum()
```




    14.699995258300001



## Part 2.2 - Optimization with Newton-Raphson 

In Part 2.2, we are going to use the Newton-Raphson method to optimize the same logistic regression model. To do so, we will need to 1) implement the `logistic_hessian` function to compute the Hessian matrix of logistic loss for the given data set, and 2) use `scipy`'s `optimize` function to perform the optimization, rather than writing a function by hand to do so.  


```python
def logistic_hessian(w, X, y):

    # compute the Hessian of the log-loss error (matrix) with respect
    # to w (vector) for the given data X and y                               
    #
    # Inputs:
    # w = d x 1
    # X = N x d
    # y = N x 1
    # Output:
    # Hessian = d x d matrix
    
    print(w.shape)
    if len(w.shape) == 1:
        w = w[:,np.newaxis]
    print(w.shape)
    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE
    hessian = np.zeros((X.shape[1],X.shape[1]))
    for i in range(0,len(X)):
        wt=np.transpose(w)
        num = np.exp(np.dot(np.dot(y[i],wt),X[i]))
        den = (1+np.exp(np.dot(np.dot(y[i],wt),X[i])))
        den = np.dot(den,den)
        hessian = hessian + np.dot(np.dot((num/den),(X[i])),np.transpose(X[i]))
    hessian = hessian/len(X)
    return hessian
```


```python
from scipy.optimize import minimize

def run_newton_raphson(X,y):
    args = (X,y[:,np.newaxis])
    opts = {'maxiter' : 50}    # Preferred value.    
    w_init = np.zeros((X.shape[1]))
    print(w_init.shape)
    # note: this is almost what you need, you just need to figure out what arguments are necessary here!
    soln = minimize(fun=logistic_objective,
                    x0=w_init,
                    jac=logistic_gradient,
                    hess=logistic_hessian,
                    args=args,
                    method='Newton-CG',
                    options=opts)

    w = np.transpose(np.array(soln.x))
    w = w[:,np.newaxis]
    return w
```


```python
run_newton_raphson(X,y)
```

    (3,)
    (3,)
    (3, 1)





    array([[0.],
           [0.],
           [0.]])


